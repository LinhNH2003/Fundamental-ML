# Mini-Project for Fundamentals of Machine Learning Course
![background](./materials/ai_wp.jpg)
This repository contains the code and data for a mini-project on facial expression recognition using machine learning algorithms.

## ğŸ“‘ Project Policy
- Team: group should consist of 3-4 students.

    |No.| Student Name    | Student ID |
    | --------| -------- | ------- |
    |1|Nguyá»…n HoÃ i Linh|21280097|
    |2|Tráº§n Thá»‹ BÃ­ch Tuyá»n|21280059|
    |3|Nguyá»…n Thá»‹ Yáº¿n NhÆ°|21280082|
   |4|Tráº§n Trá»‹nh Mai Vy|21280122|

- The submission deadline is strict: **11:59 PM** on **June 22nd, 2024**. Commits pushed after this deadline will not be considered.

## ğŸ“¦ Project Structure

The repository is organized into the following directories:

- **/data**: This directory contains the facial expression dataset. You'll need to download the dataset and place it here before running the notebooks. (Download link provided below)
- **/notebooks**: This directory contains the Jupyter notebook ```EDA.ipynb```. This notebook guides you through exploratory data analysis (EDA) and classification tasks.

## âš™ï¸ Usage

This project is designed to be completed in the following steps:

1. **Fork the Project**: Click on the ```Fork``` button on the top right corner of this repository, this will create a copy of the repository in your own GitHub account. Complete the table at the top by entering your team member names.

2. **Download the Dataset**: Download the facial expression dataset from the following [link](https://mega.nz/file/foM2wDaa#GPGyspdUB2WV-fATL-ZvYj3i4FqgbVKyct413gxg3rE) and place it in the **/data** directory:

3. **Complete the Tasks**: Open the ```notebooks/EDA.ipynb``` notebook in your Jupyter Notebook environment. The notebook is designed to guide you through various tasks, including:
    
    1. Prerequisite
    3. Principle Component Analysis
    4. Image Classification
    5. Evaluating Classification Performance 

    Make sure to run all the code cells in the ```EDA.ipynb``` notebook and ensure they produce output before committing and pushing your changes.

5. **Commit and Push Your Changes**: Once you've completed the tasks outlined in the notebook, commit your changes to your local repository and push them to your forked repository on GitHub.

## ğŸ¯MINI PROJECTğŸ¯
### ğŸ¥‡I. Tá»•ng quan vá» dá»¯ liá»‡u
- Project nháº­n dáº¡ng biá»ƒu cáº£m khuÃ´n máº·t Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn cÃ¡c hÃ¬nh áº£nh khuÃ´n máº·t tá»« bá»™ dá»¯ liá»‡u
- Táº­p dá»¯ liá»‡u Ä‘Æ°á»£c tá»•ng há»£p tá»« internet, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ phÃ¢n loáº¡i biá»ƒu hiá»‡n khuÃ´n máº·t. Dá»¯ liá»‡u bao gá»“m cÃ¡c hÃ¬nh áº£nh thang Ä‘á»™ xÃ¡m cá»§a khuÃ´n máº·t, má»—i hÃ¬nh áº£nh cÃ³ kÃ­ch thÆ°á»›c 48x48 pixel. CÃ¡c khuÃ´n máº·t Ä‘Ã£ Ä‘Æ°á»£c tá»± Ä‘á»™ng cÄƒn chá»‰nh Ä‘á»ƒ gáº§n nhÆ° á»Ÿ giá»¯a vÃ  chiáº¿m má»™t khu vá»±c tÆ°Æ¡ng tá»± trong má»—i hÃ¬nh áº£nh.
- Má»¥c tiÃªu lÃ  sá»­ dá»¥ng nhá»¯ng thuáº­t toÃ¡n Machine Learning vÃ  Deep Learning Ä‘á»ƒ phÃ¢n loáº¡i tá»«ng khuÃ´n máº·t dá»±a trÃªn cáº£m xÃºc Ä‘Æ°á»£c thá»ƒ hiá»‡n, gÃ¡n nÃ³ vÃ o má»™t trong báº£y loáº¡i cáº£m xÃºc:
    | Categorie | Emotion  |
    |-----------|----------|
    | 0         | Angry    |
    | 1         | Disgust  |
    | 2         | Fear     |
    | 3         | Happy    |
    | 4         | Sad      |
    | 5         | Surprise |
    | 6         | Neutral  |
#### 1. Prerequisite
##### **1.1 ThÃ´ng tin vá» dá»¯ liá»‡u:**
- Káº¿t quáº£ thu Ä‘Æ°á»£c tá»« data: táº­p dá»¯ liá»‡u bao gá»“m 35,887 dÃ²ng vÃ  2 cá»™t: emotion vÃ  pixels. Cá»™t emotion chá»©a cÃ¡c giÃ¡ trá»‹ sá»‘ nguyÃªn Ä‘áº¡i diá»‡n cho cÃ¡c loáº¡i cáº£m xÃºc, vÃ  cá»™t pixels chá»©a cÃ¡c chuá»—i kÃ½ tá»± Ä‘áº¡i diá»‡n cho cÃ¡c giÃ¡ trá»‹ pixel cá»§a hÃ¬nh áº£nh.
  ![background](./materials/view.png)
- **CÃ¡c nhÃ£n dá»¯ liá»‡u:**
   + NhÃ£n Angry:
     ![background](./materials/Angry.png)
   + NhÃ£n Disgust:
     ![background](./materials/Disgust.png)
   + NhÃ£n Fear:
      ![background](./materials/Fear.png)
   + NhÃ£n Happy:
      ![background](./materials/Happy.png)
   + NhÃ£n Sad:
      ![background](./materials/Sad.png)
   + NhÃ£n Surprise:
      ![background](./materials/Surpise.png)
   + NhÃ£n Neutral:
     ![background](./materials/Neutral.png)
##### **1.2 Thá»±c hiá»‡n cÃ¡c xá»­ lÃ­ ban Ä‘áº§u:**
- Kiá»ƒm tra giÃ¡ trá»‹ thiáº¿u vÃ  dá»¯ liá»‡u trÃ¹ng láº·p
- Khi Ä‘Ã³ ta tháº¥y ráº±ng dá»¯ liá»‡u Ä‘Æ°á»£c cung cáº¥p khÃ´ng cÃ³ giÃ¡ trá»‹ thiáº¿u vÃ  cÃ³ **1793** giÃ¡ trá»‹ trÃ¹ng láº·p => cáº§n drop giÃ¡ trá»‹ trÃ¹ng láº·p.
##### **1.3 Trá»±c quan dá»¯ liá»‡u ban Ä‘áº§u:**
  - Tá»•ng quan pháº§n trÄƒm cÃ¡c nhÃ£n trong dá»¯ liá»‡u:
     ![background](./materials/PB_emotion.png) 
  - Nháº­n xÃ©t vá» phÃ¢n phá»‘i nhÃ£n dá»¯ liá»‡u
    ![background](./materials/phanphoidulieu.png) 
    - **Train Labels**: ta nháº­n tháº¥y sá»± máº¥t cÃ¢n báº±ng rÃµ rá»‡t:
        - NhÃ£n "Happy" xuáº¥t hiá»‡n nhiá»u nháº¥t (~8000).
        - NhÃ£n "Disgust" xuáº¥t hiá»‡n Ã­t nháº¥t (gáº§n 0).
        - NhÃ£n "Fear", "Sad", vÃ  "Neutral" trung bÃ¬nh (~5000-7000).
        - NhÃ£n "Angry" vÃ  "Surprise" tháº¥p hÆ¡n.
    - **Random Labels** cÃ³ phÃ¢n phá»‘i Ä‘á»“ng Ä‘á»u: má»i nhÃ£n cÃ³ táº§n suáº¥t xuáº¥t hiá»‡n tÆ°Æ¡ng Ä‘Æ°Æ¡ng (~5000).
    - **So sÃ¡nh chung**
        - **Train Labels**: Máº¥t cÃ¢n báº±ng giá»¯a cÃ¡c nhÃ£n.
        - **Random Labels**: PhÃ¢n phá»‘i Ä‘á»u Ä‘áº·n.
    => **Káº¿t luáº­n**: Dá»¯ liá»‡u máº¥t cÃ¢n báº±ng trong táº­p huáº¥n luyá»‡n bá»Ÿi vÃ¬ nhÃ£n "Happy" quÃ¡ phá»• biáº¿n cÃ³ thá»ƒ gÃ¢y thiÃªn vá»‹ cho mÃ´ hÃ¬nh.
### ğŸ¥ˆ**II. Principal Components Analysis - PCA**
- Má»™t trong nhá»¯ng á»©ng dá»¥ng phá»• biáº¿n nháº¥t cá»§a biáº¿n Ä‘á»•i dá»¯ liá»‡u khÃ´ng giÃ¡m sÃ¡t lÃ  giáº£m chiá»u dá»¯ liá»‡u. QuÃ¡ trÃ¬nh nÃ y giáº£m sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng (chiá»u) trong dá»¯ liá»‡u. Khi dá»¯ liá»‡u cÃ³ sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng lá»›n, viá»‡c phÃ¢n tÃ­ch cÃ³ thá»ƒ tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n vÃ  khÃ³ khÄƒn. CÃ¡c ká»¹ thuáº­t giáº£m chiá»u dá»¯ liá»‡u giÃºp kháº¯c phá»¥c nhá»¯ng thÃ¡ch thá»©c nÃ y.

- PhÃ¢n TÃ­ch ThÃ nh Pháº§n ChÃ­nh (PCA) lÃ  má»™t ká»¹ thuáº­t phá»• biáº¿n cho viá»‡c giáº£m chiá»u dá»¯ liá»‡u. NÃ³ biáº¿n Ä‘á»•i dá»¯ liá»‡u thÃ nh má»™t táº­p há»£p má»›i cá»§a cÃ¡c Ä‘áº·c trÆ°ng gá»i lÃ  cÃ¡c thÃ nh pháº§n chÃ­nh (PCs). Nhá»¯ng PCs nÃ y Ä‘Æ°á»£c sáº¯p xáº¿p theo thá»© tá»± quan trá»ng, náº¯m báº¯t cÃ¡c biáº¿n thá»ƒ quan trá»ng nháº¥t trong dá»¯ liá»‡u. Báº±ng cÃ¡ch chá»n má»™t táº­p há»£p con cá»§a nhá»¯ng PCs thÃ´ng tin nháº¥t, chÃºng ta cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c sá»± giáº£m kÃ­ch thÆ°á»›c dá»¯ liá»‡u Ä‘Ã¡ng ká»ƒ trong khi váº«n giá»¯ láº¡i thÃ´ng tin cá»‘t yáº¿u cho viá»‡c phÃ¢n tÃ­ch.

#### **CÃ¢u há»i 1: Can you visualize the data projected onto two principal components?**
- HÃ¬nh áº£nh khi trá»±c quan dá»¯ liá»‡u vá» 2 chiá»u:
![background](./materials/pca.png)
**Nháº­n xÃ©t**: Tá»« hÃ¬nh áº£nh trÃªn cÃ³ thá»ƒ tháº¥y chÃºng ta khÃ´ng thá»ƒ trá»±c quan dá»¯ liá»‡u trÃªn khÃ´ng gian hai chiá»u(n_componets = 2) vÃ¬ khÃ´ng mang láº¡i Ã½ nghÄ©a nÃ o vá» máº·t trá»±c quan Ä‘á»‘i vá»›i bá»™ dá»¯ liá»‡u.
- HÃ¬nh áº£nh khi trá»±c quan dá»¯ liá»‡u vá» 2 chiá»u vá»›i 2 label:
![background](./materials/2D-2.png)
=>**PhÃ¢n phá»‘i dá»¯ liá»‡u:** Trong táº¥t cáº£ cÃ¡c biá»ƒu Ä‘á»“, cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u cá»§a hai nhÃ³m (mÃ u xanh vÃ  mÃ u cam) Ä‘á»u phÃ¢n bá»‘ khÃ¡ Ä‘á»u nhau, cho tháº¥y ráº±ng cÃ¡c thÃ nh pháº§n chÃ­nh khÃ´ng thá»ƒ hoÃ n toÃ n tÃ¡ch biá»‡t cÃ¡c nhÃ³m nÃ y má»™t cÃ¡ch rÃµ rÃ ng. Äiá»u nÃ y cÃ³ thá»ƒ ngá»¥ Ã½ ráº±ng cÃ¡c nhÃ³m dá»¯ liá»‡u nÃ y cÃ³ sá»± chá»“ng chÃ©o lá»›n trong khÃ´ng gian Ä‘áº·c trÆ°ng PCA
#### **CÃ¢u há»i 2:How to determine the optimal number of principal components using pca.explained_variance_? Explain your selection process.**
- Äá»ƒ xÃ¡c Ä‘á»‹nh sá»‘ lÆ°á»£ng thÃ nh pháº§n chÃ­nh (principal components - PCs) tá»‘i Æ°u, ta cáº§n xem xÃ©t tá»· lá»‡ phÆ°Æ¡ng sai Ä‘Æ°á»£c giáº£i thÃ­ch (explained_variance_ratio_) bá»Ÿi cÃ¡c PCs. Má»¥c tiÃªu lÃ  chá»n sá»‘ lÆ°á»£ng PCs sao cho giá»¯ láº¡i Ä‘Æ°á»£c pháº§n lá»›n phÆ°Æ¡ng sai trong dá»¯ liá»‡u ban Ä‘áº§u mÃ  khÃ´ng cáº§n sá»­ dá»¥ng quÃ¡ nhiá»u PCs, giÃºp giáº£m chiá»u dá»¯ liá»‡u vÃ  cáº£i thiá»‡n hiá»‡u quáº£ tÃ­nh toÃ¡n.
- CÃ¡c bÆ°á»›c thá»±c hiá»‡n:
  + 1. **Khá»Ÿi táº¡o PCA vÃ  tÃ­nh toÃ¡n:** Khá»Ÿi táº¡o Ä‘á»‘i tÆ°á»£ng PCA vÃ  Ã¡p dá»¥ng nÃ³ lÃªn dá»¯ liá»‡u Ä‘á»ƒ 
    tÃ­nh toÃ¡n cÃ¡c PCs.
  + 2. **TÃ­nh toÃ¡n phÆ°Æ¡ng sai tÃ­ch lÅ©y**: Sá»­ dá»¥ng 
    ```np.cumsum(pca.explained_variance_ratio_)``` Ä‘á»ƒ tÃ­nh toÃ¡n tá»· lá»‡ phÆ°Æ¡ng sai tÃ­ch lÅ©y 
    Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi cÃ¡c PCs.
  + 3. **Váº½ Scree Plot**: Táº¡o biá»ƒu Ä‘á»“ Scree Plot Ä‘á»ƒ trá»±c quan hÃ³a tá»· lá»‡ phÆ°Æ¡ng sai tÃ­ch 
    lÅ©y theo sá»‘ lÆ°á»£ng PCs.
  + 4. **XÃ¡c Ä‘á»‹nh Ä‘iá»ƒm khuá»·u (elbow point)**: Sá»­ dá»¥ng logic Ä‘á»ƒ tÃ¬m Ä‘iá»ƒm khuá»·u, lÃ  Ä‘iá»ƒm mÃ  
    sau Ä‘Ã³ tá»· lá»‡ tÄƒng cá»§a phÆ°Æ¡ng sai giáº£i thÃ­ch giáº£m Ä‘i Ä‘Ã¡ng ká»ƒ. ÄÃ¢y thÆ°á»ng lÃ  dáº¥u hiá»‡u Ä‘á»ƒ 
    chá»n sá»‘ lÆ°á»£ng PCs tá»‘i Æ°u.

  ![background](./materials/ellow.png)
  **Nháº­n xÃ©t**: Tá»« hÃ¬nh áº£nh cÃ³ thá»ƒ tháº¥y component tá»‘i Æ°u Ä‘Æ°á»£c chá»n lÃ  **104**. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  104 PCs Ä‘áº§u tiÃªn giáº£i thÃ­ch Ä‘Æ°á»£c 90% phÆ°Æ¡ng sai cá»§a dá»¯ liá»‡u ban Ä‘áº§u.
### ğŸ¥‰**III.Model**
#### 1.1 Chuáº©n bá»‹ dá»¯ liá»‡u trÆ°á»›c khi apply model
##### a) **Chia táº­p dá»¯ liá»‡u thÃ nh train vÃ  test**
- ÄÃ¢y lÃ  bÆ°á»›c chia dá»¯ liá»‡u ban Ä‘áº§u thÃ nh hai pháº§n riÃªng biá»‡t Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh vÃ  Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a nÃ³. Táº­p huáº¥n luyá»‡n Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ mÃ´ hÃ¬nh há»c tá»« dá»¯ liá»‡u, trong khi táº­p kiá»ƒm tra dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh sau khi huáº¥n luyá»‡n.\
- Vá»›i dá»¯ liá»‡u nÃ y táº­p train vÃ  test Ä‘Ã£ chia theo test_size=0.2 vÃ  random_state=42.
- Sau khi chia dá»¯ liá»‡u, cÃ¡c giÃ¡ trá»‹ dá»¯ liá»‡u thÃ´ Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh Ä‘á»‹nh dáº¡ng phÃ¹ há»£p Ä‘á»ƒ sá»­ dá»¥ng trong mÃ´ hÃ¬nh. VÃ­ dá»¥, trong trÆ°á»ng há»£p nÃ y, dá»¯ liá»‡u pixel ban Ä‘áº§u Ä‘Æ°á»£c chuyá»ƒn tá»« chuá»—i thÃ nh cÃ¡c máº£ng sá»‘ nguyÃªn Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ hiá»ƒu vÃ  xá»­ lÃ½.

##### b) **Biáº¿n Ä‘á»•i dá»¯ liá»‡u**
Sau khi chia dá»¯ liá»‡u, cÃ¡c giÃ¡ trá»‹ dá»¯ liá»‡u thÃ´ Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh Ä‘á»‹nh dáº¡ng phÃ¹ há»£p Ä‘á»ƒ sá»­ dá»¥ng trong mÃ´ hÃ¬nh. VÃ­ dá»¥, trong trÆ°á»ng há»£p nÃ y, dá»¯ liá»‡u pixel ban Ä‘áº§u Ä‘Æ°á»£c chuyá»ƒn tá»« chuá»—i thÃ nh cÃ¡c máº£ng sá»‘ nguyÃªn Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ hiá»ƒu vÃ  xá»­ lÃ½.

XÃ¢y dá»±ng cÃ¡c model vá»›i 3 dáº¡ng dá»¯ liá»‡u Ä‘Æ°á»£c xá»­ lÃ½, gá»“m:
- ###### **Dá»¯ liá»‡u gá»‘c**
- ###### **Dá»¯ liá»‡u giáº£m chiá»u báº±ng PCA:**
```python
from sklearn.decomposition import PCA
# Chá»n sá»‘ thÃ nh pháº§n chÃ­nh sao cho giá»¯ láº¡i Ã­t nháº¥t 90% phÆ°Æ¡ng sai cá»§a dá»¯ liá»‡u gá»‘c
pca = PCA(n_components=104)
```
Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c biáº¿n Ä‘á»•i báº±ng PCA (X_test_pca) Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh mÃ¡y há»c sau khi Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u Ä‘Ã£ giáº£m chiá»u (X_train_pca). Viá»‡c nÃ y giÃºp Ä‘áº£m báº£o ráº±ng mÃ´ hÃ¬nh Ä‘ang Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃªn cÃ¡c dá»¯ liá»‡u cÃ³ cÃ¹ng phÃ¢n phá»‘i vá»›i dá»¯ liá»‡u huáº¥n luyá»‡n, sau khi Ä‘Ã£ Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p giáº£m chiá»u dá»¯ liá»‡u PCA.

- ###### **Dá»¯ liá»‡u inverse tá»« PCA vá»›i áº£nh Ä‘Æ°á»£c loáº¡i bá» bá»›t nhá»¯ng features khÃ´ng quan trá»ng:**
Tiáº¿p theo ta  sá»­ dá»¥ng hÃ m inverse Ä‘á»ƒ phá»¥c há»“i dá»¯ liá»‡u tá»« khÃ´ng gian giáº£m chiá»u (thÃ´ng qua PCA) vá» khÃ´ng gian ban Ä‘áº§u. Nháº±m Ä‘á»ƒ Ä‘Ã¡nh giÃ¡:
- ÄÃ¡nh giÃ¡ láº¡i cháº¥t lÆ°á»£ng cá»§a viá»‡c giáº£m chiá»u dá»¯ liá»‡u.
- Äá»‘i chiáº¿u vÃ  so sÃ¡nh cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u giá»¯a khÃ´ng gian ban Ä‘áº§u vÃ  khÃ´ng gian Ä‘Ã£ giáº£m chiá»u.
- Sá»­ dá»¥ng dá»¯ liá»‡u phá»¥c há»“i Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng hiá»‡u quáº£ trÃªn cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u gá»‘c, khÃ´ng chá»‰ trÃªn cÃ¡c Ä‘iá»ƒm Ä‘Ã£ giáº£m chiá»u
```python
X_train_restored = pca.inverse_transform(X_train_pca)
X_test_restored = pca.inverse_transform(X_test_pca)
```
#### 1.2 Model 
##### a) Model GradientBoostingClassifier
 ![background](./materials/image.png)
Gradient boosting sá»­ dá»¥ng má»™t quy trÃ¬nh láº·p láº¡i Ä‘á»ƒ cáº­p nháº­t mÃ´ hÃ¬nh. CÃ´ng thá»©c tá»•ng quÃ¡t cho mÃ´ hÃ¬nh á»Ÿ bÆ°á»›c \( m \) lÃ :\
$$F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)$$ 

Trong Ä‘Ã³:
- $F_m(x)$lÃ  mÃ´ hÃ¬nh táº¡i bÆ°á»›c \( m \).
- $F_{m-1}(x)$lÃ  mÃ´ hÃ¬nh táº¡i bÆ°á»›c trÆ°á»›c Ä‘Ã³.
- $(\nu)$ lÃ  tá»‘c Ä‘á»™ há»c (learning rate), má»™t há»‡ sá»‘ giáº£m Ä‘á»ƒ Ä‘iá»u chá»‰nh má»©c Ä‘á»™ áº£nh hÆ°á»Ÿng cá»§a má»—i cÃ¢y.
- $(h_m(x))$ lÃ  cÃ¢y quyáº¿t Ä‘á»‹nh má»›i Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ dá»± Ä‘oÃ¡n pháº§n dÆ° (residuals) hoáº·c gradient cá»§a hÃ m máº¥t mÃ¡t táº¡i bÆ°á»›c \(m\).

QuÃ¡ trÃ¬nh huáº¥n luyá»‡n bao gá»“m cÃ¡c bÆ°á»›c sau:
1. Khá»Ÿi táº¡o mÃ´ hÃ¬nh vá»›i má»™t giÃ¡ trá»‹ khÃ´ng Ä‘á»•i:
   $$F_0(x) = \arg\min_\gamma \sum_{i=1}^{N} L(y_i,\gamma)$$
   Trong Ä‘Ã³, $L$ lÃ  hÃ m máº¥t mÃ¡t vÃ  $(y_i)$ lÃ  nhÃ£n thá»±c táº¿.
2. Vá»›i má»—i bÆ°á»›c $(m = 1)$ Ä‘áº¿n $M$:\
   a. TÃ­nh gradient cá»§a hÃ m máº¥t mÃ¡t:
   $$g_im = [âˆ‚L(y_i, F(x_i)) / âˆ‚F(x_i)]  táº¡i  F(x) = F_{m-1}(x)$$
   b. Huáº¥n luyá»‡n cÃ¢y quyáº¿t Ä‘á»‹nh $h_m(x)$ Ä‘á»ƒ dá»± Ä‘oÃ¡n gradient $g_{im}$.\
   c. Cáº­p nháº­t mÃ´ hÃ¬nh:\
      $F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)$\
QuÃ¡ trÃ¬nh nÃ y tiáº¿p tá»¥c cho Ä‘áº¿n khi sá»‘ bÆ°á»›c \( M \) Ä‘Æ°á»£c hoÃ n thÃ nh hoáº·c mÃ´ hÃ¬nh Ä‘áº¡t Ä‘áº¿n hiá»‡u suáº¥t mong muá»‘n.
Thá»±c hiá»‡n Grid search Ä‘á»ƒ tÃ¬m ra hyperparameter tá»‘t nháº¥t vÃ  Ã¡p dá»¥ng vá»›i:
- Model vá»›i dá»¯ liá»‡u gá»‘c:
    - Best parameters found:  **{'learning_rate': 0.25, 'max_features': 48, 'n_estimators': 200}**
    - Vá»›i Accuracy of the best Gradient Boosting model cao nháº¥t lÃ :  **0.4098841472356651**
- Model vá»›i dá»¯ liá»‡u PCA
    - Best parameters found:  **{'learning_rate': 0.1, 'max_features': 24, 'n_estimators': 200}**
    - Accuracy of the best Gradient Boosting model: **0.37879454465464146**
- Model vá»›i dá»¯ liá»‡u inverse
    - Best parameters found: **{'learning_rate': 0.25, 'max_features': 48, 'n_estimators': 200}**
    - Accuracy of the best Gradient Boosting model: **0.39917876521484086**
##### b) Model XGBoost Classifier
![background](./materials/xgb.png)
Äáº·c Ä‘iá»ƒm chÃ­nh cá»§a XGBoost.
**Tá»‘i Æ°u hÃ³a tá»‘c Ä‘á»™ vÃ  hiá»‡u suáº¥t:**
* Tree Pruning: Sá»­ dá»¥ng chiáº¿n lÆ°á»£c cáº¯t tá»‰a cÃ¢y (pruning) Ä‘á»ƒ trÃ¡nh overfitting, vá»›i ká»¹ thuáº­t "maximum depth".
* Parallel Processing: Há»— trá»£ xá»­ lÃ½ song song Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ huáº¥n luyá»‡n.
* Cache-aware Access: Tá»‘i Æ°u hÃ³a truy cáº­p bá»™ nhá»› cache Ä‘á»ƒ tÄƒng hiá»‡u suáº¥t tÃ­nh toÃ¡n. \

**Regularization (Äiá»u chá»‰nh):** \
* XGBoost thÃªm cÃ¡c thuáº­t ngá»¯ Ä‘iá»u chá»‰nh vÃ o hÃ m máº¥t mÃ¡t Ä‘á»ƒ trÃ¡nh overfitting, cá»¥ thá»ƒ lÃ  L1 (lasso) vÃ  L2 (ridge) regularization.

**Handling Missing Values:**
* Tá»± Ä‘á»™ng xá»­ lÃ½ cÃ¡c giÃ¡ trá»‹ thiáº¿u (missing values) báº±ng cÃ¡ch há»c cÃ¡c con Ä‘Æ°á»ng tá»‘i Æ°u trong cÃ¢y quyáº¿t Ä‘á»‹nh.

**Shrinkage (Learning Rate):**
* Há»— trá»£ giáº£m tá»‘c Ä‘á»™ há»c (learning rate), giÃºp lÃ m cháº­m quÃ¡ trÃ¬nh há»c vÃ  cáº£i thiá»‡n kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cá»§a mÃ´ hÃ¬nh.

**Early Stopping:**
* XGBoost há»— trá»£ dá»«ng sá»›m khi mÃ´ hÃ¬nh khÃ´ng cÃ²n cáº£i thiá»‡n trÃªn táº­p kiá»ƒm tra, giÃºp tiáº¿t kiá»‡m thá»i gian vÃ  trÃ¡nh overfitting.

**Cross Validation:**
* Há»— trá»£ k-fold cross validation tÃ­ch há»£p Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh vÃ  chá»n siÃªu tham sá»‘ tá»‘i Æ°u.
- Model vá»›i dá»¯ liá»‡u gá»‘c:
    + Best parameters found:  **{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 700}**
    + Accuracy of the best XGBoosing model: **0.5050153246029535**
- Model vá»›i dá»¯ liá»‡u PCA
    + Best parameters found: **{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 700}**
    + Accuracy of the best XGBoosing model: **0.4555586514349401**
- Model vá»›i dá»¯ liá»‡u inverse
    + Best parameters found: **{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 700}**
    + Accuracy of the best XGBoosing model: **0.5032042351629981**
##### c) Model Logistic Regreesion
![background](./materials/LGT.png)
**Logistic Regression** lÃ  má»™t trong nhá»¯ng thuáº­t toÃ¡n quan trá»ng trong há»c mÃ¡y, Ä‘áº·c biá»‡t lÃ  trong bÃ i toÃ¡n phÃ¢n loáº¡i. Thuáº­t toÃ¡n nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ dá»± Ä‘oÃ¡n xÃ¡c suáº¥t cá»§a má»™t biáº¿n phá»¥ thuá»™c nhá»‹ phÃ¢n dá»±a trÃªn cÃ¡c biáº¿n Ä‘á»™c láº­p.

**Äáº·c Ä‘iá»ƒm chÃ­nh cá»§a Logistic Regression**:
- **Loáº¡i thuáº­t toÃ¡n**: Thuáº­t toÃ¡n há»c cÃ³ giÃ¡m sÃ¡t (supervised learning).
- **Loáº¡i bÃ i toÃ¡n**: PhÃ¢n loáº¡i nhá»‹ phÃ¢n (binary classification).
- **Äáº§u ra**: Dá»± Ä‘oÃ¡n xÃ¡c suáº¥t rÆ¡i vÃ o má»™t trong hai lá»›p (0 hoáº·c 1).
- **HÃ m kÃ­ch hoáº¡t**: Sigmoid function, Ä‘Æ°a Ä‘áº§u ra vá» má»™t giÃ¡ trá»‹ trong khoáº£ng (0, 1)

**CÃ´ng thá»©c chÃ­nh cá»§a Logistic Regression:**
CÃ´ng thá»©c dá»± Ä‘oÃ¡n cá»§a Logistic Regression cho má»™t Ä‘iá»ƒm dá»¯ liá»‡u x lÃ :

$$\hat{p} = \sigma(\mathbf{w}^T \mathbf{x} + b)$$

Trong Ä‘Ã³:
$\hat p$: lÃ  xÃ¡c suáº¥t dá»± Ä‘oÃ¡n.
**$Ïƒ$** lÃ  hÃ m sigmoid function: $ \sigma(z) = \frac{1}{1 + e^{-z}} $

**$w$** lÃ  vector trá»ng sá»‘.

**$x$** lÃ  vector Ä‘áº·c trÆ°ng cá»§a dá»¯ liá»‡u.

**$b$** lÃ  há»‡ sá»‘ cháº·n (bias).

Thá»±c hiá»‡n Grid search Ä‘á»ƒ tÃ¬m ra hyperparameter tá»‘t nháº¥t vÃ  Ã¡p dá»¥ng vá»›i:


##### d) Model SVM
![background](./materials/svm.png) 

Thuáº­t toÃ¡n SVM (Support Vector Machine) thá»±c hiá»‡n phÃ¢n lá»›p dá»±a trÃªn cÃ¡c nguyÃªn lÃ½ toÃ¡n há»c nháº±m tÃ¬m ra siÃªu pháº³ng tá»‘i Æ°u Ä‘á»ƒ phÃ¢n chia cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u cá»§a hai lá»›p. CÃ¡c bÆ°á»›c thá»±c hiá»‡n thuáº­t toÃ¡n SVM:
- **Chuáº©n bá»‹ dá»¯ liá»‡u**

Cho má»™t táº­p dá»¯ liá»‡u huáº¥n luyá»‡n $(x_i, y_i)$ vá»›i $i = 1, \ldots, n$, trong Ä‘Ã³ $x_i \in \mathbb{R}^d$ lÃ  Ä‘iá»ƒm dá»¯ liá»‡u vÃ  $y_i \in \{-1, 1\}$ lÃ  nhÃ£n lá»›p.

- **Thiáº¿t láº­p hÃ m quyáº¿t Ä‘á»‹nh**

SiÃªu pháº³ng cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi phÆ°Æ¡ng trÃ¬nh:
\[ $w\cdot x + b = 0$ \]
trong Ä‘Ã³ $w$ lÃ  vector trá»ng sá»‘ vÃ  $b$ lÃ  háº±ng sá»‘ bias.

- **RÃ ng buá»™c phÃ¢n lá»›p**

Äá»ƒ dá»¯ liá»‡u Ä‘Æ°á»£c phÃ¢n lá»›p chÃ­nh xÃ¡c, cáº§n thoáº£ mÃ£n:
\[ $y_i (w \cdot x_i + b) \geq 1 \$]
Äiá»u nÃ y Ä‘áº£m báº£o ráº±ng cÃ¡c Ä‘iá»ƒm thuá»™c lá»›p +1 náº±m má»™t phÃ­a cá»§a siÃªu pháº³ng vÃ  cÃ¡c Ä‘iá»ƒm thuá»™c lá»›p -1 náº±m phÃ­a bÃªn kia.

- **HÃ m má»¥c tiÃªu**

Má»¥c tiÃªu lÃ  tá»‘i Ä‘a hÃ³a khoáº£ng cÃ¡ch giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u vÃ  siÃªu pháº³ng. Äiá»u nÃ y tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c tá»‘i thiá»ƒu hÃ³a:
\[ $\frac{1}{2} \| w \|^2 $\]
dÆ°á»›i cÃ¡c rÃ ng buá»™c:
\[ $y_i (w \cdot x_i + b) \geq 1$ \]

- **BÃ i toÃ¡n tá»‘i Æ°u hÃ³a**

ÄÃ¢y lÃ  bÃ i toÃ¡n tá»‘i Æ°u hÃ³a báº­c hai vá»›i cÃ¡c rÃ ng buá»™c tuyáº¿n tÃ­nh, cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t láº¡i dÆ°á»›i dáº¡ng:
\[ $\min_{w,b} \frac{1}{2} \| w \|^2 $\]
\[ $\text{subject to } y_i (w \cdot x_i + b) \geq 1, \; i = 1, \ldots, n$ \]

- **Sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p Lagrange**

Äá»ƒ giáº£i bÃ i toÃ¡n nÃ y, ta sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p nhÃ¢n tá»­ Lagrange:
\[ $L(w, b, \alpha) = \frac{1}{2} \| w \|^2 - \sum_{i=1}^n \alpha_i [y_i (w \cdot x_i + b) - 1]$ \]
trong Ä‘Ã³ $\alpha_i \geq 0$ lÃ  cÃ¡c nhÃ¢n tá»­ Lagrange.

- **TÃ¬m nghiá»‡m tá»‘i Æ°u**

Äá»ƒ tÃ¬m nghiá»‡m cá»§a $L(w, b, \alpha)$, ta cáº§n tá»‘i thiá»ƒu hÃ³a $L$ theo $w$ vÃ  $b$ vÃ  tá»‘i Ä‘a hÃ³a theo $\alpha$. Äiá»u nÃ y dáº«n Ä‘áº¿n há»‡ phÆ°Æ¡ng trÃ¬nh:
\[ $\frac{\partial L}{\partial w} = 0 \Rightarrow w = \sum_{i=1}^n \alpha_i y_i x_i$ \]
\[ $\frac{\partial L}{\partial b} = 0 \Rightarrow \sum_{i=1}^n \alpha_i y_i = 0 $\]

- **Dual Problem**

Báº±ng cÃ¡ch thay tháº¿ $w$ vÃ  $b$ vÃ o $L$, ta cÃ³ bÃ i toÃ¡n tá»‘i Æ°u kÃ©p:
\[ $\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^n \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) $\]
\[ $\text{subject to } \sum_{i=1}^n \alpha_i y_i = 0$ \]
\[ $\alpha_i \geq 0, \; i = 1, \ldots, n$ \]

- **Kernel Trick (Náº¿u dá»¯ liá»‡u khÃ´ng tuyáº¿n tÃ­nh)**
Náº¿u dá»¯ liá»‡u khÃ´ng tuyáº¿n tÃ­nh, ta cÃ³ thá»ƒ sá»­ dá»¥ng kernel trick Ä‘á»ƒ chuyá»ƒn dá»¯ liá»‡u vÃ o khÃ´ng gian Ä‘áº·c trÆ°ng cao hÆ¡n:
\[$ K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j) $\]
trong Ä‘Ã³ $\phi$ lÃ  hÃ m Ã¡nh xáº¡ vÃ o khÃ´ng gian Ä‘áº·c trÆ°ng.

- **XÃ¡c Ä‘á»‹nh siÃªu pháº³ng vÃ  hÃ m quyáº¿t Ä‘á»‹nh**
Sau khi giáº£i quyáº¿t Ä‘Æ°á»£c bÃ i toÃ¡n tá»‘i Æ°u, vector trá»ng sá»‘ $w$ vÃ  háº±ng sá»‘ bias $b$ cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh hÃ m quyáº¿t Ä‘á»‹nh:
\[ $f(x) = w \cdot x + b$ \]
Dá»± Ä‘oÃ¡n lá»›p cá»§a Ä‘iá»ƒm dá»¯ liá»‡u má»›i $x$ dá»±a trÃªn dáº¥u cá»§a $f(x)$:
\[ $\text{class}(x) = \text{sign}(f(x)) $\]

Thá»±c hiá»‡n Grid search Ä‘á»ƒ tÃ¬m ra hyperparameter tá»‘t nháº¥t vÃ  Ã¡p dá»¥ng vá»›i:\
Best parameters found:**svm__C=10, svm__gamma=scale**
- Model vá»›i dá»¯ liá»‡u gá»‘c:
    + Accuracy: **0.4754806352744497**
- Model vá»›i dá»¯ liá»‡u PCA
    + Accuracy: **0.4660072443577598**
- Model vá»›i dá»¯ liá»‡u inverse
    + Accuracy: **0.4831429367511842**
##### e) Model Multi-layer Perceptron classifier
![background](./materials/mlp.png)
Cáº¥u trÃºc má»™t neuron.
* Má»™t neuron sáº½ Ä‘Æ°á»£c má»™t hoáº·c nhiá»u Ä‘áº§u vÃ o x<sub>i</sub>. CÃ¡c Ä‘áº§u vÃ o Ä‘Ã³ng vai
trÃ² nhÆ° cÃ¡c sá»£i nhÃ¡nh tháº§n kinh (dendrites) nháº­n tÃ­nh hiá»‡u tá»« cÃ¡c
neuron khÃ¡c.
* CÃ¡c giÃ¡ trá»‹ Ä‘áº§u vÃ o x<sub>i</sub> Ä‘Æ°á»£c Ä‘iá»u phá»‘i táº§m áº£nh hÆ°á»Ÿng bá»Ÿi cÃ¡c trá»ng sá»‘ tÆ°Æ¡ng á»©ng w<sub>i</sub> cá»§a nÃ³, thá»ƒ hiá»‡n má»©c Ä‘á»™ quan trá»ng (Ä‘á»™ máº¡nh) cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘á»‘i vá»›i quÃ¡ trÃ¬nh xá»­ lÃ­ thÃ´ng tin (quÃ¡ trÃ¬nh chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u tá»« nÃºt nÃ y sang nÃºt
khÃ¡c).
* Táº¡i má»—i nÃºt, chÃºng ta sáº½ biáº¿n Ä‘á»•i nhá»¯ng dá»¯ liá»‡u Ä‘áº§u vÃ o nÃ y báº±ng
cÃ¡ch tÃ­nh tá»•ng cÃ¡c giÃ¡ trá»‹ Ä‘áº§u vÃ o vá»›i trá»ng sá»‘ liÃªn káº¿t tÆ°Æ¡ng á»©ng trÃªn
cÃ¡c Ä‘áº§u vÃ o.
* GiÃ¡ trá»‹ Ä‘á»™ lá»‡ch (bias).
NgoÃ i cÃ¡c trá»ng sá»‘, má»™t thÃ nh pháº§n khÃ¡c Ä‘Æ°á»£c Ã¡p dá»¥ng cho thÃ´ng
tin Ä‘áº§u vÃ o, Ä‘Æ°á»£c gá»i lÃ  giÃ¡ trá»‹ Ä‘áº§u vÃ o (bias). NÃ³ Ä‘Æ°á»£c thÃªm vÃ o káº¿t quáº£ cá»§a phÃ©p nhÃ¢n trá»ng sá»‘ vá»›i giÃ¡ trá»‹ Ä‘áº§u vÃ o.
Bá»Ÿi vÃ¬ káº¿t quáº£ cá»§a phÃ©p nhÃ¢n luÃ´n lÃ  má»™t hÃ m tuyáº¿n tÃ­nh Ä‘i qua gá»‘c
tá»a Ä‘á»™ nÃªn viá»‡c sá»­ dá»¥ng bias giÃºp máº¡ng neuron cÃ³ thá»ƒ dá»‹ch chuyá»ƒn
hÃ m tuyáº¿n tÃ­nh nÃ y má»™t cÃ¡ch linh hoáº¡t hÆ¡n Ä‘á»ƒ mÃ´ hÃ¬nh khá»›p vá»›i dá»¯
liá»‡u Ä‘Æ°á»£c huáº¥n luyá»‡n.
* HÃ m kÃ­ch hoáº¡t (Activation function)
LÃ  má»™t thÃ nh pháº§n ráº¥t quan trá»ng trong mÃ´ hÃ¬nh máº¡ng neuron. NÃ³
quyáº¿t Ä‘á»‹nh khi nÃ o má»™t neuron Ä‘Æ°á»£c kÃ­ch hoáº¡t hay khÃ´ng Ä‘Æ°á»£c kÃ­ch
hoáº¡t. Liá»‡u thÃ´ng tin mÃ  neuron nháº­n Ä‘Æ°á»£c cÃ³ liÃªn quan Ä‘áº¿n thÃ´ng tin
Ä‘Æ°á»£c Ä‘Æ°a ra hay nÃªn bá» qua dá»±a trÃªn káº¿t quáº£ tá»•ng há»£p cÃ¡c tÃ­n hiá»‡u
Ä‘áº§u vÃ o mÃ  nÃ³ nháº­n Ä‘Æ°á»£c tá»« cÃ¡c neuron trÆ°á»›c Ä‘Ã³.
Náº¿u khÃ´ng cÃ³ hÃ m kÃ­ch hoáº¡t thÃ¬ trá»ng sá»‘ liÃªn káº¿t vÃ  bias chá»‰ Ä‘Æ¡n giáº£n
nhÆ° 1 hÃ m biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh. Giáº£i 1 hÃ m tuyáº¿n tÃ­nh sáº½ Ä‘Æ¡n giáº£n hÆ¡n
nhiá»u nhÆ°ng sáº½ khÃ³ cÃ³ thá»ƒ mÃ´ hÃ¬nh hÃ³a vÃ  giáº£i Ä‘Æ°á»£c nhá»¯ng váº¥n Ä‘á»
phá»©c táº¡p.

Thá»±c hiá»‡n Grid search Ä‘á»ƒ tÃ¬m ra hyperparameter tá»‘t nháº¥t vÃ  Ã¡p dá»¥ng vá»›i:
- Model vá»›i dá»¯ liá»‡u gá»‘c:
    + Best parameters found:  **{'alpha': 0.0001, 'hidden_layer_sizes': (1024, 512, 256, 128), 'learning_rate_init': 0.001}**
    + Test accuracy: **0.4653106714962385**
- Model vá»›i dá»¯ liá»‡u PCA:
    + Test accuracy: **0.43981610476455835**
- Model vá»›i dá»¯ liá»‡u inverse:
    + Test accuracy: **0.4580663137364168**
### **IV.Evaluating Classification Performance**
- **Compare the performance of the different classification models using various metrics: accuracy, precision, recall, and F1-score.
Based on the evaluation metrics, explain which model performs best and why. Identify the emotion category where the model makes the most accurate and most errors. (1 point)**
    - Commpa
| Data Type | Gradient Boosting | Logistic Regression | XGBoost  | MLP      | SVM      |
| --------- | ----------------- | ------------------- | -------- | -------- | -------- |
| Original  | 0.409884          | 0.312656            | 0.505015 | 0.465311 | 0.475481 |
| PCA       | 0.378795          | 0.363690            | 0.455559 | 0.439816 | 0.466007 |
| Restored  | 0.385394          | 0.360903            | 0.503204 | 0.458066 | 0.483143 |


