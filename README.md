# Mini-Project for Fundamentals of Machine Learning Course
![background](./materials/ai_wp.jpg)
This repository contains the code and data for a mini-project on facial expression recognition using machine learning algorithms.

## ğŸ“‘ Project Policy
- Team: group should consist of 3-4 students.

    |No.| Student Name    | Student ID |
    | --------| -------- | ------- |
    |1|Nguyá»…n HoÃ i Linh|21280097|
    |2|Tráº§n Trá»‹nh Mai Vy|21280122|
    |3|Tráº§n Thá»‹ BÃ­ch Tuyá»n|21280059|
    |4|Nguyá»…n Thá»‹ Yáº¿n NhÆ°|21280082|

- The submission deadline is strict: **11:59 PM** on **June 22nd, 2024**. Commits pushed after this deadline will not be considered.

## ğŸ“¦ Project Structure

The repository is organized into the following directories:

- **/data**: This directory contains the facial expression dataset. You'll need to download the dataset and place it here before running the notebooks. (Download link provided below)
- **/notebooks**: This directory contains the Jupyter notebook ```EDA.ipynb```. This notebook guides you through exploratory data analysis (EDA) and classification tasks.

## âš™ï¸ Usage

This project is designed to be completed in the following steps:

1. **Fork the Project**: Click on the ```Fork``` button on the top right corner of this repository, this will create a copy of the repository in your own GitHub account. Complete the table at the top by entering your team member names.

2. **Download the Dataset**: Download the facial expression dataset from the following [link](https://mega.nz/file/foM2wDaa#GPGyspdUB2WV-fATL-ZvYj3i4FqgbVKyct413gxg3rE) and place it in the **/data** directory:

3. **Complete the Tasks**: Open the ```notebooks/EDA.ipynb``` notebook in your Jupyter Notebook environment. The notebook is designed to guide you through various tasks, including:
    
    1. Prerequisite
    3. Principle Component Analysis
    4. Image Classification
    5. Evaluating Classification Performance 

    Make sure to run all the code cells in the ```EDA.ipynb``` notebook and ensure they produce output before committing and pushing your changes.

5. **Commit and Push Your Changes**: Once you've completed the tasks outlined in the notebook, commit your changes to your local repository and push them to your forked repository on GitHub.

## MINI PROJECT
### I.Tá»•ng quan vá» Ä‘á»“ Ã¡n
- Project nháº­n dáº¡ng biá»ƒu cáº£m khuÃ´n máº·t Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn cÃ¡c hÃ¬nh áº£nh khuÃ´n máº·t tá»« bá»™ dá»¯ liá»‡u
- Táº­p dá»¯ liá»‡u Ä‘Æ°á»£c tá»•ng há»£p tá»« internet, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ phÃ¢n loáº¡i biá»ƒu hiá»‡n khuÃ´n máº·t. Dá»¯ liá»‡u bao gá»“m cÃ¡c hÃ¬nh áº£nh thang Ä‘á»™ xÃ¡m cá»§a khuÃ´n máº·t, má»—i hÃ¬nh áº£nh cÃ³ kÃ­ch thÆ°á»›c 48x48 pixel. CÃ¡c khuÃ´n máº·t Ä‘Ã£ Ä‘Æ°á»£c tá»± Ä‘á»™ng cÄƒn chá»‰nh Ä‘á»ƒ gáº§n nhÆ° á»Ÿ giá»¯a vÃ  chiáº¿m má»™t khu vá»±c tÆ°Æ¡ng tá»± trong má»—i hÃ¬nh áº£nh.
- Má»¥c tiÃªu lÃ  sá»­ dá»¥ng nhá»¯ng thuáº­t toÃ¡n Machine Learning vÃ  Deep Learning Ä‘á»ƒ phÃ¢n loáº¡i tá»«ng khuÃ´n máº·t dá»±a trÃªn cáº£m xÃºc Ä‘Æ°á»£c thá»ƒ hiá»‡n, gÃ¡n nÃ³ vÃ o má»™t trong báº£y loáº¡i cáº£m xÃºc (0=Tá»©c giáº­n, 1=GhÃª tá»Ÿm, 2=Sá»£ hÃ£i, 3=Vui váº», 4=Buá»“n, 5=Báº¥t ngá», 6=Trung láº­p).
#### 1. Prerequisite
##### **1.1 ThÃ´ng tin vá» dá»¯ liá»‡u:**
- Káº¿t quáº£ thu Ä‘Æ°á»£c tá»« data: táº­p dá»¯ liá»‡u bao gá»“m 35,887 dÃ²ng vÃ  2 cá»™t: emotion vÃ  pixels. Cá»™t emotion chá»©a cÃ¡c giÃ¡ trá»‹ sá»‘ nguyÃªn Ä‘áº¡i diá»‡n cho cÃ¡c loáº¡i cáº£m xÃºc, vÃ  cá»™t pixels chá»©a cÃ¡c chuá»—i kÃ½ tá»± Ä‘áº¡i diá»‡n cho cÃ¡c giÃ¡ trá»‹ pixel cá»§a hÃ¬nh áº£nh.
  ![background](./materials/view.png)
- **CÃ¡c nhÃ£n dá»¯ liá»‡u:**
   + NhÃ£n Angry:
     ![background](./materials/Angry.png)
   + NhÃ£n Disgust:
     ![background](./materials/Disgust.png)
   + NhÃ£n Fear:
      ![background](./materials/Fear.png)
   + NhÃ£n Happy:
      ![background](./materials/Happy.png)
   + NhÃ£n Sad:
      ![background](./materials/Sad.png)
   + NhÃ£n Surprise:
      ![background](./materials/Surpise.png)
   + NhÃ£n Neutral:
     ![background](./materials/Neutral.png)
##### **1.2 Thá»±c hiá»‡n cÃ¡c xá»­ lÃ­ ban Ä‘áº§u:**
- Kiá»ƒm tra giÃ¡ trá»‹ thiáº¿u vÃ  dá»¯ liá»‡u trÃ¹ng láº·p
- Khi Ä‘Ã³ ta tháº¥y ráº±ng dá»¯ liá»‡u Ä‘Æ°á»£c cung cáº¥p khÃ´ng cÃ³ giÃ¡ trá»‹ thiáº¿u vÃ  cÃ³ **1793** giÃ¡ trá»‹ trÃ¹ng láº·p => cáº§n drop giÃ¡ trá»‹ trÃ¹ng láº·p.
##### **1.3 Trá»±c quan dá»¯ liá»‡u ban Ä‘áº§u:**
  - Tá»•ng quan pháº§n trÄƒm cÃ¡c nhÃ£n trong dá»¯ liá»‡u:
     ![background](./materials/PB_emotion.png) 
  - Nháº­n xÃ©t vá» phÃ¢n phá»‘i nhÃ£n dá»¯ liá»‡u
    ![background](./materials/phanphoidulieu.png) 
    - **Train Labels**: ta nháº­n tháº¥y sá»± máº¥t cÃ¢n báº±ng rÃµ rá»‡t:
        - NhÃ£n "Happy" xuáº¥t hiá»‡n nhiá»u nháº¥t (~8000).
        - NhÃ£n "Disgust" xuáº¥t hiá»‡n Ã­t nháº¥t (gáº§n 0).
        - NhÃ£n "Fear", "Sad", vÃ  "Neutral" trung bÃ¬nh (~5000-7000).
        - NhÃ£n "Angry" vÃ  "Surprise" tháº¥p hÆ¡n.
    - **Random Labels** cÃ³ phÃ¢n phá»‘i Ä‘á»“ng Ä‘á»u: má»i nhÃ£n cÃ³ táº§n suáº¥t xuáº¥t hiá»‡n tÆ°Æ¡ng Ä‘Æ°Æ¡ng (~5000).
    - **So sÃ¡nh chung**
        - **Train Labels**: Máº¥t cÃ¢n báº±ng giá»¯a cÃ¡c nhÃ£n.
        - **Random Labels**: PhÃ¢n phá»‘i Ä‘á»u Ä‘áº·n.
    => **Káº¿t luáº­n**: Dá»¯ liá»‡u máº¥t cÃ¢n báº±ng trong táº­p huáº¥n luyá»‡n bá»Ÿi vÃ¬ nhÃ£n "Happy" quÃ¡ phá»• biáº¿n cÃ³ thá»ƒ gÃ¢y thiÃªn vá»‹ cho mÃ´ hÃ¬nh.
### IIII. Principal Components Analysis - PCA
- Má»™t trong nhá»¯ng á»©ng dá»¥ng phá»• biáº¿n nháº¥t cá»§a biáº¿n Ä‘á»•i dá»¯ liá»‡u khÃ´ng giÃ¡m sÃ¡t lÃ  giáº£m chiá»u dá»¯ liá»‡u. QuÃ¡ trÃ¬nh nÃ y giáº£m sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng (chiá»u) trong dá»¯ liá»‡u. Khi dá»¯ liá»‡u cÃ³ sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng lá»›n, viá»‡c phÃ¢n tÃ­ch cÃ³ thá»ƒ tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n vÃ  khÃ³ khÄƒn. CÃ¡c ká»¹ thuáº­t giáº£m chiá»u dá»¯ liá»‡u giÃºp kháº¯c phá»¥c nhá»¯ng thÃ¡ch thá»©c nÃ y.

- PhÃ¢n TÃ­ch ThÃ nh Pháº§n ChÃ­nh (PCA) lÃ  má»™t ká»¹ thuáº­t phá»• biáº¿n cho viá»‡c giáº£m chiá»u dá»¯ liá»‡u. NÃ³ biáº¿n Ä‘á»•i dá»¯ liá»‡u thÃ nh má»™t táº­p há»£p má»›i cá»§a cÃ¡c Ä‘áº·c trÆ°ng gá»i lÃ  cÃ¡c thÃ nh pháº§n chÃ­nh (PCs). Nhá»¯ng PCs nÃ y Ä‘Æ°á»£c sáº¯p xáº¿p theo thá»© tá»± quan trá»ng, náº¯m báº¯t cÃ¡c biáº¿n thá»ƒ quan trá»ng nháº¥t trong dá»¯ liá»‡u. Báº±ng cÃ¡ch chá»n má»™t táº­p há»£p con cá»§a nhá»¯ng PCs thÃ´ng tin nháº¥t, chÃºng ta cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c sá»± giáº£m kÃ­ch thÆ°á»›c dá»¯ liá»‡u Ä‘Ã¡ng ká»ƒ trong khi váº«n giá»¯ láº¡i thÃ´ng tin cá»‘t yáº¿u cho viá»‡c phÃ¢n tÃ­ch.

#### **CÃ¢u há»i 1: Can you visualize the data projected onto two principal components?**
- HÃ¬nh áº£nh khi trá»±c quan dá»¯ liá»‡u vá» 2 chiá»u:
![background](./materials/pca.png)
**Nháº­n xÃ©t**: Tá»« hÃ¬nh áº£nh trÃªn cÃ³ thá»ƒ tháº¥y chÃºng ta khÃ´ng thá»ƒ trá»±c quan dá»¯ liá»‡u trÃªn khÃ´ng gian hai chiá»u(n_componets = 2) vÃ¬ khÃ´ng mang láº¡i Ã½ nghÄ©a nÃ o vá» máº·t trá»±c quan Ä‘á»‘i vá»›i bá»™ dá»¯ liá»‡u.

#### **CÃ¢u há»i 2:How to determine the optimal number of principal components using pca.explained_variance_? Explain your selection process.**
- Äá»ƒ xÃ¡c Ä‘á»‹nh sá»‘ lÆ°á»£ng thÃ nh pháº§n chÃ­nh (principal components - PCs) tá»‘i Æ°u, ta cáº§n xem xÃ©t tá»· lá»‡ phÆ°Æ¡ng sai Ä‘Æ°á»£c giáº£i thÃ­ch (explained_variance_ratio_) bá»Ÿi cÃ¡c PCs. Má»¥c tiÃªu lÃ  chá»n sá»‘ lÆ°á»£ng PCs sao cho giá»¯ láº¡i Ä‘Æ°á»£c pháº§n lá»›n phÆ°Æ¡ng sai trong dá»¯ liá»‡u ban Ä‘áº§u mÃ  khÃ´ng cáº§n sá»­ dá»¥ng quÃ¡ nhiá»u PCs, giÃºp giáº£m chiá»u dá»¯ liá»‡u vÃ  cáº£i thiá»‡n hiá»‡u quáº£ tÃ­nh toÃ¡n.
- CÃ¡c bÆ°á»›c thá»±c hiá»‡n:
  + 1. **Khá»Ÿi táº¡o PCA vÃ  tÃ­nh toÃ¡n:** Khá»Ÿi táº¡o Ä‘á»‘i tÆ°á»£ng PCA vÃ  Ã¡p dá»¥ng nÃ³ lÃªn dá»¯ liá»‡u Ä‘á»ƒ 
    tÃ­nh toÃ¡n cÃ¡c PCs.
  + 2. **TÃ­nh toÃ¡n phÆ°Æ¡ng sai tÃ­ch lÅ©y**: Sá»­ dá»¥ng 
    ```np.cumsum(pca.explained_variance_ratio_)``` Ä‘á»ƒ tÃ­nh toÃ¡n tá»· lá»‡ phÆ°Æ¡ng sai tÃ­ch lÅ©y 
    Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi cÃ¡c PCs.
  + 3. **Váº½ Scree Plot**: Táº¡o biá»ƒu Ä‘á»“ Scree Plot Ä‘á»ƒ trá»±c quan hÃ³a tá»· lá»‡ phÆ°Æ¡ng sai tÃ­ch 
    lÅ©y theo sá»‘ lÆ°á»£ng PCs.
  + 4. **XÃ¡c Ä‘á»‹nh Ä‘iá»ƒm khuá»·u (elbow point)**: Sá»­ dá»¥ng logic Ä‘á»ƒ tÃ¬m Ä‘iá»ƒm khuá»·u, lÃ  Ä‘iá»ƒm mÃ  
    sau Ä‘Ã³ tá»· lá»‡ tÄƒng cá»§a phÆ°Æ¡ng sai giáº£i thÃ­ch giáº£m Ä‘i Ä‘Ã¡ng ká»ƒ. ÄÃ¢y thÆ°á»ng lÃ  dáº¥u hiá»‡u Ä‘á»ƒ 
    chá»n sá»‘ lÆ°á»£ng PCs tá»‘i Æ°u.

  ![background](./materials/ellow.png)
  **Nháº­n xÃ©t**: Tá»« hÃ¬nh áº£nh cÃ³ thá»ƒ tháº¥y component tá»‘i Æ°u Ä‘Æ°á»£c chá»n lÃ  **104**. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  104 PCs Ä‘áº§u tiÃªn giáº£i thÃ­ch Ä‘Æ°á»£c 90% phÆ°Æ¡ng sai cá»§a dá»¯ liá»‡u ban Ä‘áº§u.
### **III.Model**
#### 1.1 Biáº¿n Ä‘á»•i dá»¯ liá»‡u trÆ°á»›c khi apply model
##### a) **Chia táº­p dá»¯ liá»‡u thÃ nh train vÃ  test**
- ÄÃ¢y lÃ  bÆ°á»›c chia dá»¯ liá»‡u ban Ä‘áº§u thÃ nh hai pháº§n riÃªng biá»‡t Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh vÃ  Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a nÃ³. Táº­p huáº¥n luyá»‡n Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ mÃ´ hÃ¬nh há»c tá»« dá»¯ liá»‡u, trong khi táº­p kiá»ƒm tra dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh sau khi huáº¥n luyá»‡n.\
- Vá»›i dá»¯ liá»‡u nÃ y táº­p train vÃ  test Ä‘Ã£ chia theo test_size=0.2 vÃ  random_state=42.
- Sau khi chia dá»¯ liá»‡u, cÃ¡c giÃ¡ trá»‹ dá»¯ liá»‡u thÃ´ Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh Ä‘á»‹nh dáº¡ng phÃ¹ há»£p Ä‘á»ƒ sá»­ dá»¥ng trong mÃ´ hÃ¬nh. VÃ­ dá»¥, trong trÆ°á»ng há»£p nÃ y, dá»¯ liá»‡u pixel ban Ä‘áº§u Ä‘Æ°á»£c chuyá»ƒn tá»« chuá»—i thÃ nh cÃ¡c máº£ng sá»‘ nguyÃªn Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ hiá»ƒu vÃ  xá»­ lÃ½.

##### b) **Biáº¿n Ä‘á»•i dá»¯ liá»‡u**
Sau khi chia dá»¯ liá»‡u, cÃ¡c giÃ¡ trá»‹ dá»¯ liá»‡u thÃ´ Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh Ä‘á»‹nh dáº¡ng phÃ¹ há»£p Ä‘á»ƒ sá»­ dá»¥ng trong mÃ´ hÃ¬nh. VÃ­ dá»¥, trong trÆ°á»ng há»£p nÃ y, dá»¯ liá»‡u pixel ban Ä‘áº§u Ä‘Æ°á»£c chuyá»ƒn tá»« chuá»—i thÃ nh cÃ¡c máº£ng sá»‘ nguyÃªn Ä‘á»ƒ mÃ´ hÃ¬nh cÃ³ thá»ƒ hiá»ƒu vÃ  xá»­ lÃ½.
```python
from sklearn.decomposition import PCA
# Chá»n sá»‘ thÃ nh pháº§n chÃ­nh sao cho giá»¯ láº¡i Ã­t nháº¥t 90% phÆ°Æ¡ng sai cá»§a dá»¯ liá»‡u gá»‘c
pca = PCA(n_components=104)
```
Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c biáº¿n Ä‘á»•i báº±ng PCA (X_test_pca) Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh mÃ¡y há»c sau khi Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u Ä‘Ã£ giáº£m chiá»u (X_train_pca). Viá»‡c nÃ y giÃºp Ä‘áº£m báº£o ráº±ng mÃ´ hÃ¬nh Ä‘ang Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃªn cÃ¡c dá»¯ liá»‡u cÃ³ cÃ¹ng phÃ¢n phá»‘i vá»›i dá»¯ liá»‡u huáº¥n luyá»‡n, sau khi Ä‘Ã£ Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p giáº£m chiá»u dá»¯ liá»‡u PCA.
##### c) **Inverse data from pca**
Tiáº¿p theo ta  sá»­ dá»¥ng hÃ m inverse Ä‘á»ƒ phá»¥c há»“i dá»¯ liá»‡u tá»« khÃ´ng gian giáº£m chiá»u (thÃ´ng qua PCA) vá» khÃ´ng gian ban Ä‘áº§u. Nháº±m Ä‘á»ƒ Ä‘Ã¡nh giÃ¡:
- ÄÃ¡nh giÃ¡ láº¡i cháº¥t lÆ°á»£ng cá»§a viá»‡c giáº£m chiá»u dá»¯ liá»‡u.
- Äá»‘i chiáº¿u vÃ  so sÃ¡nh cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u giá»¯a khÃ´ng gian ban Ä‘áº§u vÃ  khÃ´ng gian Ä‘Ã£ giáº£m chiá»u.
- Sá»­ dá»¥ng dá»¯ liá»‡u phá»¥c há»“i Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng hiá»‡u quáº£ trÃªn cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u gá»‘c, khÃ´ng chá»‰ trÃªn cÃ¡c Ä‘iá»ƒm Ä‘Ã£ giáº£m chiá»u
```python
X_train_restored = pca.inverse_transform(X_train_pca)
X_test_restored = pca.inverse_transform(X_test_pca)
```
#### 1.2 Model 
##### a) Model GradientBoostingClassifier
 ![background](./materials/image.png)
Gradient boosting sá»­ dá»¥ng má»™t quy trÃ¬nh láº·p láº¡i Ä‘á»ƒ cáº­p nháº­t mÃ´ hÃ¬nh. CÃ´ng thá»©c tá»•ng quÃ¡t cho mÃ´ hÃ¬nh á»Ÿ bÆ°á»›c \( m \) lÃ :\
$$F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)$$ 

Trong Ä‘Ã³:
- $F_m(x)$lÃ  mÃ´ hÃ¬nh táº¡i bÆ°á»›c \( m \).
- $F_{m-1}(x)$lÃ  mÃ´ hÃ¬nh táº¡i bÆ°á»›c trÆ°á»›c Ä‘Ã³.
- $(\nu)$ lÃ  tá»‘c Ä‘á»™ há»c (learning rate), má»™t há»‡ sá»‘ giáº£m Ä‘á»ƒ Ä‘iá»u chá»‰nh má»©c Ä‘á»™ áº£nh hÆ°á»Ÿng cá»§a má»—i cÃ¢y.
- $(h_m(x))$ lÃ  cÃ¢y quyáº¿t Ä‘á»‹nh má»›i Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ dá»± Ä‘oÃ¡n pháº§n dÆ° (residuals) hoáº·c gradient cá»§a hÃ m máº¥t mÃ¡t táº¡i bÆ°á»›c \(m\).

QuÃ¡ trÃ¬nh huáº¥n luyá»‡n bao gá»“m cÃ¡c bÆ°á»›c sau:
1. Khá»Ÿi táº¡o mÃ´ hÃ¬nh vá»›i má»™t giÃ¡ trá»‹ khÃ´ng Ä‘á»•i:
   $F_0(x) = \arg\min_\gamma \sum_{i=1}^{N} L(y_i,\gamma)$
   Trong Ä‘Ã³, $L$ lÃ  hÃ m máº¥t mÃ¡t vÃ  $(y_i)$ lÃ  nhÃ£n thá»±c táº¿.
2. Vá»›i má»—i bÆ°á»›c $(m = 1)$ Ä‘áº¿n $M$:\
   a. TÃ­nh gradient cá»§a hÃ m máº¥t mÃ¡t:
   $$g_im = [âˆ‚L(y_i, F(x_i)) / âˆ‚F(x_i)]  táº¡i  F(x) = F_{m-1}(x)$$
   b. Huáº¥n luyá»‡n cÃ¢y quyáº¿t Ä‘á»‹nh $h_m(x)$ Ä‘á»ƒ dá»± Ä‘oÃ¡n gradient $g_{im}$.\
   c. Cáº­p nháº­t mÃ´ hÃ¬nh:\
      $F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)$
QuÃ¡ trÃ¬nh nÃ y tiáº¿p tá»¥c cho Ä‘áº¿n khi sá»‘ bÆ°á»›c \( M \) Ä‘Æ°á»£c hoÃ n thÃ nh hoáº·c mÃ´ hÃ¬nh Ä‘áº¡t Ä‘áº¿n hiá»‡u suáº¥t mong muá»‘n.
- Model vá»›i dá»¯ liá»‡u gá»‘c:
    - Best parameters found:  **{'learning_rate': 0.25, 'max_features': 48, 'n_estimators': 200}**
    - Vá»›i Accuracy of the best Gradient Boosting model cao nháº¥t lÃ :  0.4098841472356651
- Model vá»›i dá»¯ liá»‡u PCA
    - Best parameters found:  **{'learning_rate': 0.1, 'max_features': 24, 'n_estimators': 200}**
    - Accuracy of the best Gradient Boosting model: 0.37879454465464146
- Model vá»›i dá»¯ liá»‡u inverse
    - Best parameters found: **{'learning_rate': 0.25, 'max_features': 48, 'n_estimators': 200}**
    - Accuracy of the best Gradient Boosting model: 0.39917876521484086
##### b) Model XGBoost Classifier
![background](./materials/xgb.png)
Äáº·c Ä‘iá»ƒm chÃ­nh cá»§a XGBoost.

**Tá»‘i Æ°u hÃ³a tá»‘c Ä‘á»™ vÃ  hiá»‡u suáº¥t:**
* Tree Pruning: Sá»­ dá»¥ng chiáº¿n lÆ°á»£c cáº¯t tá»‰a cÃ¢y (pruning) Ä‘á»ƒ trÃ¡nh overfitting, vá»›i ká»¹ thuáº­t "maximum depth".
* Parallel Processing: Há»— trá»£ xá»­ lÃ½ song song Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ huáº¥n luyá»‡n.
* Cache-aware Access: Tá»‘i Æ°u hÃ³a truy cáº­p bá»™ nhá»› cache Ä‘á»ƒ tÄƒng hiá»‡u suáº¥t tÃ­nh toÃ¡n. \

**Regularization (Äiá»u chá»‰nh):** \
* XGBoost thÃªm cÃ¡c thuáº­t ngá»¯ Ä‘iá»u chá»‰nh vÃ o hÃ m máº¥t mÃ¡t Ä‘á»ƒ trÃ¡nh overfitting, cá»¥ thá»ƒ lÃ  L1 (lasso) vÃ  L2 (ridge) regularization.

**Handling Missing Values:**
* Tá»± Ä‘á»™ng xá»­ lÃ½ cÃ¡c giÃ¡ trá»‹ thiáº¿u (missing values) báº±ng cÃ¡ch há»c cÃ¡c con Ä‘Æ°á»ng tá»‘i Æ°u trong cÃ¢y quyáº¿t Ä‘á»‹nh.

**Shrinkage (Learning Rate):**
* Há»— trá»£ giáº£m tá»‘c Ä‘á»™ há»c (learning rate), giÃºp lÃ m cháº­m quÃ¡ trÃ¬nh há»c vÃ  cáº£i thiá»‡n kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cá»§a mÃ´ hÃ¬nh.

**Early Stopping:**
* XGBoost há»— trá»£ dá»«ng sá»›m khi mÃ´ hÃ¬nh khÃ´ng cÃ²n cáº£i thiá»‡n trÃªn táº­p kiá»ƒm tra, giÃºp tiáº¿t kiá»‡m thá»i gian vÃ  trÃ¡nh overfitting.

**Cross Validation:**
* Há»— trá»£ k-fold cross validation tÃ­ch há»£p Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh vÃ  chá»n siÃªu tham sá»‘ tá»‘i Æ°u.
##### c) Model Logistic Regreesion
##### d) Model SVM
##### e) Model Multi-layer Perceptron classifier








